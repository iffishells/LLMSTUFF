# LLMSTUFF

# Ollama Model Setup
1. Following the link [ollama](https://ollama.com/download)
2. install ollama using following command : ``` curl -fsSL https://ollama.com/install.sh | sh ```
3. Pull the model According to your usage : ``` ollama models pull llama3.1:8b```
4. For running the ollama model on your local system ``` ollama run llama3.1:8b```

# Activate conda environment
```angular2html
conda activate llm_chatbot
```